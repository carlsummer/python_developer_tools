[首要学习对象](https://github.com/lorenmt/shape-adaptor) --- https://arxiv.org/pdf/2008.00892
https://github.com/NVlabs/pacnet
https://github.com/DingXiaoH/ACNet
https://github.com/xingchenzhang/RGB-T-fusion-tracking-papers-and-results
https://github.com/Megvii-BaseDetection/DynamicRouting
https://github.com/FishYuLi/BalancedGroupSoftmax
https://github.com/theFoxofSky/ddfnet
https://github.com/zhengye1995/Tianchi-2019-Guangdong-Intelligent-identification-of-cloth-defects-rank5
https://github.com/formyfamily/DenseGAP
https://github.com/MendelXu/ANN
https://github.com/dvlab-research/SA-AutoAug
https://github.com/YimianDai/open-aff
https://github.com/jakc4103/DFQ
https://github.com/Eric-mingjie/network-slimming
https://github.com/MadryLab/cifar10_challenge
https://arxiv.org/pdf/2201.02149 ----- https://github.com/pgruening/bio_inspired_min_nets_improve_the_performance_and_robustness_of_deep_networks
https://github.com/bobo0810/PytorchNetHub
https://mp.weixin.qq.com/s/bwu6cXnHOQZ1H-br-Ybp4A

https://github.com/google-research/google-research/tree/master/musiq----https://blog.csdn.net/weixin_44966641/article/details/119766643

# TPH-YOLOv5
> TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios
> <br/>[解释](https://mp.weixin.qq.com/s/nFoZd9ZIV2DRLiI5Gulf-A)
> <br/>[paper](https://arxiv.org/abs/2108.11539)

# Mobile-Former: Bridging MobileNet and Transformer
> <br/>[paper](https://arxiv.org/abs/2108.05895)

# Conformer
> [论文](https://arxiv.org/abs/2105.03889)
> <br/>[代码](https://github.com/pengzhiliang/Conformer)

# Space-time Mixing Attention for Video Transformer
> [论文地址](https://arxiv.org/abs/2106.05968)<br/>
> 代码地址：未开源
> 本文介绍了利用Transformer进行的视频识别问题。
> 最近Transformer在视频识别领域的尝试在识别精度方面展现出了非常不错的结果，
> 但在许多情况下，由于时间维度的额外建模，会导致显著的计算开销提升。在这项工作中，作者提出了一个视频Transformer模型，
> 该模型的复杂度与视频序列中的帧数呈线性的关系，因此与基于图像的Transformer模型相比，不会产生额外的计算开销。为了实现这一点，
> 本文的视频Transformer对 full space-time attention进行了两个方面的近似：(a)它将时间注意力限制在一个局部时间窗口，
> 并利用Transformer的深度来获得视频序列的全时间覆盖（这一点类似CNN中用小卷积核和深层结构获得全局空间建模的思想很像）。
> (b)它使用有效的时空混合来联合建模空间和时间信息，与仅空间注意模型相比没有产生任何额外的计算成本。
> 作者通过实验证明了，本文的模型在视频识别数据集上产生了非常高的精度。


# Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization
> [论文地址](https://arxiv.org/abs/2109.02401)<br/>
[代码地址](https://github.com/HLTCHKUST/VG-GPLMs)<br/>
> 多模态摘要总结(Multimodal abstractive summarization，MAS)模型总结了视频（视觉模态)及其相应的文本(文本模态）的知识，能够从互联网上的大量多模态数据中提取摘要信息（即化繁为简）。另一方面，近年来，大规模的生成预训练语言模型(generative pretrained language models，GPLMs)已被证明在文本生成任务中是有效的。然而，现有的MAS模型不能利用GPLMs强大的生成能力。为了填补这一研究空白，在本文中，作者的目标是研究两个问题：1)如何在不损害GPLMs生成能力的情况下注入视觉信息？2)在GPLMs中注入视觉信息的最佳位置在哪里？在本文中，作者提出了一种简单而有效的方法来构建视觉引导的GPMLs，使用基于注意力的附加层来聚合视觉信息，同时保持其原始预训练模型的文本生成能力。结果表明，本文的方法在How2数据集上，比以前的最佳模型超过了 5.7 ROUGE-1、5.3 ROUGE-2和5.1 ROUGE-L。此外，作者还进行了完整的消融研究，以分析各种模态融合方法和融合位置的有效性。

# Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision
> [论文地址](https://arxiv.org/abs/2102.05918)<br/>
> 学习良好的视觉和视觉语言表征对于解决计算机视觉问题(图像检索、图像分类、视频理解)是至关重要的，目前，预训练的特征在许多NLP任务中已经展现了非常大的潜力。虽然NLP中的表示学习已经可以用没有人工注释的原始文本训练，但视觉和视觉语言表示仍然严重依赖于昂贵或需要专家知识的训练数据集。对于视觉任务，特征表示的学习主要依赖具有显式的class标签的数据集，如ImageNet或OpenImages。对于视觉语言任务，一些使用广泛的数据集像Conceptual Captions、MS COCO以及CLIP都涉及到了数据收集和清洗的过程。这类数据预处理的工作严重阻碍了获得更大规模的数据集。在本文中，作者利用了超过10亿的图像文本对的噪声数据集，没有进行数据过滤或后处理步骤。基于对比学习损失，使用一个简单的双编码器结构来学习对齐图像和文本对的视觉和语言表示。作者证明了，语料库规模的巨大提升可以弥补数据内部存在的噪声，因此即使使用简单的学习方式，模型也能达到SOTA的特征表示。当本文模型的视觉表示转移到ImageNet和VTAB等分类任务时，也能取得很强的性能。对齐的视觉和语言表示支持zero-shot的图像分类，并在Flickr30K和MSCOCO图像-文本检索基准数据集上达到了SOTA的结果。

# Learning to Prompt for Vision-Language Models
> [论文地址](https://arxiv.org/abs/2109.01134)<br/>
[代码地址](https://github.com/KaiyangZhou/CoOp)<br/>（未开源）
视觉语言预训练最近成为特征学习的一种非常有潜力的替代方法。它从使用图像和离散标签来学习一组固定权重的传统学习方法，转向了使用两个独立的编码器来对齐图像和原始文本。这种训练范式能够受益于更广泛的监督来源（不仅仅是image-label对），并允许zero-shot（即不需要fine-tuning）转移到下游任务，因为视觉概念可以从自然语言中生成。在本文中，作者意识到在实践中部署这种模型的主要挑战是prompt engineering（提示符工程，即如何手工设计一个更好的prompt）。这是因为设计一个适当的提示符，特别是对于围绕类名的上下文单词，需要领域的专业知识，并且通常需要大量的时间来调优单词，因为提示符的轻微变化可能会对性能产生巨大的影响。此外，不同的下游任务需要特定的提示符设计，这进一步阻碍了部署的效率。为了克服这一挑战，作者提出了一种名为上下文优化(context optimization，CoOp)的方法。其主要思想是使用continuous representation在提示符中建模上下文，并在保持预训练学习的参数固定的同时从数据中进行端到端学习优化。这样，与任务相关的提示符设计就可以完全自动化了。在11个数据集上进行的实验表明，CoOp有效地将预训练的视觉语言模型转化为数据高效的视觉任务学习模型，只需少量样本微调就能击败手工设计的提示符，并且在使用更多样本微调时能够获得显著的性能提升（16 shots的平均涨幅约为17%，最高涨幅达到超过50%）。


# GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer
> [论文地址](https://arxiv.org/abs/2108.12630)<br/>
[代码地址](https://github.com/xueyee/GroupFormer)<br/>（未开源）
群体活动识别是一个关键而又具有挑战性的问题，其核心在于充分探索个体之间的时空交互，产生合理的群体表征。然而，以前的方法要么分别建模空间和时间信息，要么直接聚合个体特征形成群体特征。为了解决这些问题，作者提出了一种新的群体活动识别网络，称为GroupFormer。它联合建模时空上下文信息，通过聚类时空Transformer，有效地增强个体和群体的表征。具体来说，GroupFormer有三个的优点：
（1）聚类时空Transformer（Clustered Spatial-Temporal Transformer）能够增强个体表征和群体的表征；
（2）GroupFormer对时空依赖关系进行综合建模，并利用解码器建立时空信息之间的联系。
（3）利用聚类注意机制动态地将个体划分为多个聚类，以更高效地学习具有活动感知能力的语义表征。
此外，实验结果表明，该框架在Volleyball数据集和 Collective Activity数据集上优于SOTA的方法。
（完整解析后续会发在FightingCV公众号上~）

# CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification
> [论文地址](https://arxiv.org/abs/2103.14899)<br/>
[代码地址](https://github.com/IBM/CrossViT)<br/>
与卷积神经网络相比，最近出现的视觉Transformer(ViT)在图像分类方面取得了很好的结果。受此启发，在本文中，作者研究了如何学习Transformer模型中的多尺度特征表示来进行图像分类。为此，作者提出了一种双分支Transformer来组合不同大小的图像patch，以产生更强的图像特征。本文的方法用两个不同计算复杂度的独立分支来处理小patch的token和大patch的token，然后这些token通过attention机制进行多次的交互以更好的融合信息。此外，为了减少计算量，作者开发了一个简单而有效的基于cross-attention的token融合模块。在每一个分支中，它使用单个token（即 [CLS] token）作为query，与其他分支交换信息。本文提出cross-attention的计算复杂度和显存消耗与输入特征大小呈线性关系。实验结果表明，本文提出的CrossViT的性能优于其他基于Transformer和CNN的模型。例如，在ImageNet-1K数据集上，CrossViT比DeiT的准确率高了2%，但是FLOPs和模型参数增加的非常有限。

# Visformer: The Vision-friendly Transformer
> [论文地址](https://arxiv.org/abs/2104.12533v4)<br/>
[代码地址](https://github.com/danczs/Visformer)<br/>
目前，将基于视觉任务的Transformer结构正在快速发展。虽然一些研究人员已经证明了基于Transformer的模型具有良好的数据拟合能力，但仍有越来越多的证据表明这些模型存在过拟合，特别是在训练数据有限的情况下。本文通过逐步改变模型结构，将基于Transformer的模型逐步过渡到基于卷积的模型。在过渡过程中获得的结果，为提高视觉识别能力提供了有用的信息。基于这些观察结果，作者提出了一种名为Visformer（Vision-friendly Transformer）的新架构。在相同的计算复杂度下，Visformer在ImageNet分类精度方面，优于基于Transformer和基于卷积的模型，当模型复杂度较低或训练集较小时，优势变得更加显著。





